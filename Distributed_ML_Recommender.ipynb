{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "vital-abuse",
   "metadata": {},
   "source": [
    "# Distributed Machine Learning Recommender System"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "satisfied-arrow",
   "metadata": {},
   "source": [
    "Matheus Schmitz\n",
    "<br><a href=\"https://www.linkedin.com/in/matheusschmitz/\">LinkedIn</a></br>\n",
    "<br><a href=\"https://matheus-schmitz.github.io/\">Github brortfolio</a></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "necessary-mount",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "accurate-shuttle",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "local-attachment",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pyspark import SparkContext, SparkConf\n",
    "import time\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "import json\n",
    "import gc\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "#from textblob import TextBlob\n",
    "#from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "#from sklearn.impute import IterativeImputer\n",
    "#import sklearn.ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "delayed-finding",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Track time taken\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aerial-gravity",
   "metadata": {},
   "source": [
    "## Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "romantic-clause",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize spark\n",
    "import findspark\n",
    "findspark.init()\n",
    "sc = SparkContext.getOrCreate(SparkConf().set(\"spark.executor.memory\", \"4g\").set(\"spark.driver.memory\", \"4g\"))\n",
    "sc.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sound-watson",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "material-number",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get user inputs\n",
    "folder_path = \"data/\"\n",
    "test_file_name = \"data/yelp_val.csv\"\n",
    "output_file_name = \"predictions.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "unlimited-measurement",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data...\n",
      "Loading Data: Stage Time: 9 seconds. Total Time: 31 seconds.\n"
     ]
    }
   ],
   "source": [
    "print(f'Loading Data...')\n",
    "stage_time = time.time()\n",
    "\n",
    "# Read the CSV skipping its header, and reshape it as ((user, bizz), rating)\n",
    "trainRDD = sc.textFile(folder_path+'yelp_train.csv')\n",
    "trainHeader = trainRDD.first()\n",
    "trainRDD = trainRDD.filter(lambda row: row != trainHeader).sample(False, 0.00025).map(lambda row: row.split(',')).map(lambda row: ((row[0],row[1]), float(row[2]))).persist() #.sample(False, 0.50)\n",
    "validRDD = sc.textFile(test_file_name)\n",
    "validHeader = validRDD.first()\n",
    "validRDD = validRDD.filter(lambda row: row != validHeader).sample(False, 0.00050).map(lambda row: row.split(',')).map(lambda row: ((row[0],row[1]), float(row[2]))).persist() #.sample(False, 0.50) #,float(row[2])\n",
    "print(f'Loading Data: Stage Time: {time.time() - stage_time:.0f} seconds. Total Time: {time.time() - start_time:.0f} seconds.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indonesian-performer",
   "metadata": {},
   "source": [
    "## Encode Users and Businesses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "broke-peeing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating ID Encodings...\n",
      "Generating ID Encodings: Stage Time: 52 seconds. Total Time: 84 seconds.\n"
     ]
    }
   ],
   "source": [
    "print(f'Generating ID Encodings...')\n",
    "stage_time = time.time()\n",
    "\n",
    "# Merge RDDs to get all IDs\n",
    "mergedRDD = sc.union([trainRDD,validRDD])\n",
    "\n",
    "# Get distinct users and businesses (over train and valid datasets)\n",
    "distinct_user = mergedRDD.map(lambda row: row[0][0]).distinct().sortBy(lambda user: user).collect()\n",
    "distinct_bizz = mergedRDD.map(lambda row: row[0][1]).distinct().sortBy(lambda bizz: bizz).collect()\n",
    "\n",
    "# Convert names to IDs (to optimize memory usage when holding the values)\n",
    "user_to_encoding, encoding_to_user = {}, {}\n",
    "for encoding, real_id in enumerate(distinct_user):\n",
    "    user_to_encoding[real_id] = encoding\n",
    "    #encoding_to_user[encoding] = real_id\n",
    "\n",
    "bizz_to_encoding, encoding_to_bizz = {}, {}\n",
    "for encoding, real_id in enumerate(distinct_bizz):\n",
    "    bizz_to_encoding[real_id] = encoding\n",
    "    #encoding_to_bizz[encoding] = real_id\n",
    "\n",
    "# Use the IDs to encode the RDD, which reduces memory requirements when holding itemsets, and keep the shape as ((user, bizz), rating)\n",
    "trainRDD_enc = trainRDD.map(lambda x: ((user_to_encoding[x[0][0]], bizz_to_encoding[x[0][1]]), x[1])).persist()\n",
    "validRDD_enc = validRDD.map(lambda x: ((user_to_encoding[x[0][0]], bizz_to_encoding[x[0][1]]), x[1])).persist()\n",
    "\n",
    "# Memory management\n",
    "trainRDD.unpersist()\n",
    "del trainRDD, trainHeader, validHeader, mergedRDD\n",
    "gc.collect()\n",
    "print(f'Generating ID Encodings: Stage Time: {time.time() - stage_time:.0f} seconds. Total Time: {time.time() - start_time:.0f} seconds.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ancient-production",
   "metadata": {},
   "source": [
    "## Calculate Average Ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "taken-assurance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Average Rating Features...\n",
      "Generating Average Rating Features: Stage Time: 12 seconds. Total Time: 96 seconds.\n"
     ]
    }
   ],
   "source": [
    "print(f'Generating Average Rating Features...')\n",
    "stage_time = time.time()\n",
    "\n",
    "# Calculate average ratings \n",
    "user_avg_rating = trainRDD_enc.map(lambda x: (x[0][0], [x[1]])).reduceByKey(lambda a, b: a + b).map(lambda row: (row[0], sum(row[1]) / len(row[1]))).collectAsMap()\n",
    "bizz_avg_rating = trainRDD_enc.map(lambda x: (x[0][1], [x[1]])).reduceByKey(lambda a, b: a + b).map(lambda row: (row[0], sum(row[1]) / len(row[1]))).collectAsMap()\n",
    "print(f'Generating Average Rating Features: Stage Time: {time.time() - stage_time:.0f} seconds. Total Time: {time.time() - start_time:.0f} seconds.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extended-howard",
   "metadata": {},
   "source": [
    "## Item-Item Collaborative Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "adjusted-aruba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def item_based_CF(user, \n",
    "\t\t\t\t  bizz, \t\t\n",
    "\t\t\t\t  user_avg_rating,\n",
    "\t\t\t\t  bizz_avg_rating,\n",
    "\t\t\t\t  user_bizz_rating_dict,\n",
    "\t\t\t\t  bizz_user_rating_dict):\n",
    "\t\n",
    "\t### Ensure no errors in case a user and/or business doesn't have an average rating score ###\n",
    "\t# If both user and business have missing ratings, return the best guess, aka 3\n",
    "\tif user not in user_avg_rating and bizz not in bizz_avg_rating:\n",
    "\t\treturn ((user, bizz), 3.75) # Based on real world knowledge I know that the overall average rating is somewhere between 3.5 and 4.0\n",
    "\n",
    "\t# If only the business has a missing value, we still cannot calculate similarity, so return the average for the associated user\n",
    "\telif bizz not in bizz_avg_rating:\n",
    "\t\treturn ((user, bizz), user_avg_rating[user])\n",
    "\n",
    "\t# If only the user has a missing value, we still cannot calculate similarity, so return the average for the associated business\n",
    "\telif user not in user_avg_rating:\n",
    "\t\treturn ((user, bizz), bizz_avg_rating[bizz])\n",
    "\n",
    "\t# If both user and business have ratings, proceed to calculating similarities\n",
    "\tsimilarities = list()\n",
    "\n",
    "\t# For each business rated by the current user, calculate the similarity between the current business and the comparison business\n",
    "\tbizz_rating_dict = user_bizz_rating_dict[user]\n",
    "\tfor encoding in range(len(bizz_rating_dict)):\n",
    "\t\tpearson_corr = item_item_similarity(bizz, bizz_rating_dict[encoding][0], bizz_user_rating_dict)\n",
    "\n",
    "\t\t# Skip similarities of 0 to gain performenace\n",
    "\t\tif pearson_corr == 0:\n",
    "\t\t\tcontinue\n",
    "\n",
    "\t\tsimilarities.append((encoding, pearson_corr))\n",
    "\t\n",
    "\t# Calculate the person correlation to make a weighted prediction\n",
    "\tN = 0\n",
    "\tD = 0\n",
    "\n",
    "\tfor (encoding, pearson_corr) in similarities:\n",
    "\t\tbizz_rating_tuple = bizz_rating_dict[encoding]\n",
    "\t\tbusiness = bizz_rating_tuple[0]\n",
    "\t\trating = bizz_rating_tuple[1]\n",
    "\t\tbusiness_avg_rating = bizz_avg_rating[business]\n",
    "\t\tN += (rating - business_avg_rating) * pearson_corr\n",
    "\t\tD += abs(pearson_corr)\n",
    "\t\n",
    "\tprediction = float(bizz_avg_rating[bizz] + N/D) if N != 0 else 3.75 # Based on real world knowledge I know that the overall average rating is somewhere between 3.5 and 4.0\n",
    "\n",
    "\treturn ((user, bizz), prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "amateur-miami",
   "metadata": {},
   "outputs": [],
   "source": [
    "def item_item_similarity(curr_bizz, \n",
    "\t\t\t\t\t\t comp_bizz,\n",
    "\t\t\t\t\t\t bizz_user_rating_dict):\n",
    "\n",
    "\t# For each business get all pairs of user/rating\n",
    "\tcurr_bizz_ratings = bizz_user_rating_dict[curr_bizz]\n",
    "\tcomp_bizz_ratings = bizz_user_rating_dict[comp_bizz]\n",
    "\n",
    "\t# Get co-rated users (those who rated both businesses)\n",
    "\tcorated_users = set(curr_bizz_ratings.keys()).intersection(set(comp_bizz_ratings.keys()))\n",
    "\n",
    "\t# If there are no co-rated users, its impossible to calculate similarity, so return a guess\n",
    "\tif len(corated_users) == 0:\n",
    "\t\treturn 0.5 \n",
    "\n",
    "\t# Calculate the average rating given to the businesses by the co-rated users\n",
    "\tcurr_bizz_total = 0\n",
    "\tcomp_bizz_total = 0\n",
    "\tcount = 0\n",
    "\n",
    "\tfor user in corated_users:\n",
    "\t\tcurr_bizz_total += curr_bizz_ratings[user]\n",
    "\t\tcomp_bizz_total += comp_bizz_ratings[user]\n",
    "\t\tcount += 1\n",
    "\n",
    "\tcurr_bizz_avg = curr_bizz_total/count\n",
    "\tcomp_bizz_avg = comp_bizz_total/count\n",
    "\n",
    "\t# Calculate the pearson correlation\n",
    "\tcurr_x_comp_total = 0\n",
    "\tcurr_norm_square = 0\n",
    "\tcomp_norm_square = 0\n",
    "\n",
    "\tfor user in corated_users:\n",
    "\t\tcurr_x_comp_total += ((curr_bizz_ratings[user] - curr_bizz_avg) * (comp_bizz_ratings[user] - comp_bizz_avg))\n",
    "\t\tcurr_norm_square += (curr_bizz_ratings[user] - curr_bizz_avg)**2\n",
    "\t\tcomp_norm_square += (comp_bizz_ratings[user] - comp_bizz_avg)**2\n",
    "\n",
    "\t# Get the Pearson Correlation (Of guess a correlation if we cannot calculate the correlation for a given pair)\n",
    "\tpearson_corr = curr_x_comp_total/((curr_norm_square**0.5) * (comp_norm_square**0.5)) if curr_x_comp_total != 0 else 0.5\n",
    "\t\n",
    "\treturn pearson_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "another-request",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_dicts(dict1, dict2):\n",
    "\t# If the first element is already a dict...\n",
    "\tif type(dict1) == dict:\n",
    "\t\t# Then append the second element\n",
    "\t\tdict1.update(dict2)\n",
    "\t\treturn dict1\n",
    "\t# If it is not (because the reducer is comparing the null item to the first item)...\n",
    "\telse:\n",
    "\t\t# Then return the first item\n",
    "\t\treturn dict2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dirty-investigation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Item-Item CF Features...\n",
      "Generating Item-Item CF Features: Stage Time: 14 seconds. Total Time: 111 seconds.\n"
     ]
    }
   ],
   "source": [
    "print(f'Generating Item-Item CF Features...')\n",
    "stage_time = time.time()\n",
    "\n",
    "# For each user/bizz, get a dict with all related bizz/user and the associated rating\n",
    "user_bizz_rating_dict = trainRDD_enc.map(lambda x: (x[0][0], [(x[0][1], x[1])])).reduceByKey(lambda dict1, dict2: dict1 + dict2).collectAsMap()\n",
    "bizz_user_rating_dict = trainRDD_enc.map(lambda x: (x[0][1], {x[0][0]:x[1]})).reduceByKey(lambda dict1, dict2: merge_dicts(dict1, dict2)).collectAsMap()\n",
    "\n",
    "# Generate augmented features\n",
    "item_CF_feature_train = trainRDD_enc.map(lambda row:  item_based_CF(row[0][0],\n",
    "                                                                    row[0][1],\n",
    "                                                                    user_avg_rating,\n",
    "                                                                    bizz_avg_rating,\n",
    "                                                                    user_bizz_rating_dict,\n",
    "                                                                    bizz_user_rating_dict)).collectAsMap()\n",
    "\n",
    "item_CF_feature_test = validRDD_enc.map(lambda  row:  item_based_CF(row[0][0],\n",
    "                                                                    row[0][1],\n",
    "                                                                    user_avg_rating,\n",
    "                                                                    bizz_avg_rating,\n",
    "                                                                    user_bizz_rating_dict,\n",
    "                                                                    bizz_user_rating_dict)).collectAsMap()\n",
    "\n",
    "# Memory management\n",
    "del user_bizz_rating_dict, bizz_user_rating_dict\n",
    "gc.collect()\n",
    "print(f'Generating Item-Item CF Features: Stage Time: {time.time() - stage_time:.0f} seconds. Total Time: {time.time() - start_time:.0f} seconds.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "official-parent",
   "metadata": {},
   "source": [
    "## User & Business Complimentary Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "reflected-modification",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The categories from the Yelp dataset are available at: https://blog.yelp.com/2018/01/yelp_category_list\n",
    "yelp_categories = [\"Active Life\", \"Arts & Entertainment\", \"Automotive\", \"Beauty & Spas\", \"Education\", \"Event Planning & Services\", \n",
    "\"Financial Services\", \"Food\", \"Health & Medical\", \"Home Services\", \"Hotels & Travel\", \"Local Flavor\", \"Local Services\", \"Mass Media\", \n",
    "\"Nightlife\", \"Pets\", \"Professional Services\", \"Public Services & Government\", \"Real Estate\", \"Religious Organizations\", \"Restaurants\", \"Shopping\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "african-antenna",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting User & Business Complimentary Features...\n",
      "Extracting User & Business Complimentary Features: Stage Time: 80 seconds. Total Time: 192 seconds.\n"
     ]
    }
   ],
   "source": [
    "print(f'Extracting User & Business Complimentary Features...')\n",
    "stage_time = time.time()\n",
    "\n",
    "# Read the user and business jsons, and load the features\n",
    "user_features = sc.textFile(folder_path+'user.json').map(lambda row: json.loads(row)).filter(lambda row: row['user_id'] in distinct_user).map(lambda row: (user_to_encoding[row['user_id']], np.array([row['average_stars'], row['review_count'], int(row['yelping_since'][2:4]), row['useful'], row['funny'], row['cool'], row['fans'], len(row['friends']), len(row['elite'])], dtype=np.float16))).collectAsMap()\n",
    "bizz_features = sc.textFile(folder_path+'business.json').map(lambda row: json.loads(row)).filter(lambda row: row['business_id'] in distinct_bizz).map(lambda row: (bizz_to_encoding[row['business_id']], [row['stars'], row['review_count'], row['is_open'], row['latitude'], row['longitude'], row['categories']])).collectAsMap()\n",
    "\n",
    "# Clean the business categories keeping only those words relevant for generating features\n",
    "for key, value in bizz_features.items():\n",
    "    try:\n",
    "        bizz_features[key][5] = ' '.join([category for category in bizz_features[key][5].split(', ') if category in yelp_categories])\n",
    "    except:\n",
    "        bizz_features[key][5] = ' '\n",
    "print(f'Extracting User & Business Complimentary Features: Stage Time: {time.time() - stage_time:.0f} seconds. Total Time: {time.time() - start_time:.0f} seconds.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beautiful-consortium",
   "metadata": {},
   "source": [
    "## Extract Features From User's Tips"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cordless-completion",
   "metadata": {},
   "source": [
    "#### Train Latent Semantic Analysis Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "neutral-three",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Latent Semantic Analysis Pipeline on Tips...\n",
      "Training Latent Semantic Analysis Pipeline on Tips: Stage Time: 118 seconds. Total Time: 310 seconds.\n",
      "tips_LSA_pipeline size: 60000 -> 30\n"
     ]
    }
   ],
   "source": [
    "print(f'Training Latent Semantic Analysis Pipeline on Tips...')\n",
    "stage_time = time.time()\n",
    "all_tip_texts = sc.textFile(folder_path+'tip.json').map(lambda row: json.loads(row.replace('\\\\n', ''))).map(lambda row: row['text']).collect()\n",
    "\n",
    "# Train the Tfidftips_vectorizer and TruncatedSVD (the LSA pipeline)\n",
    "tips_vectorizer = TfidfVectorizer(ngram_range=(1, 2), max_features=9000, stop_words=None)\n",
    "tips_svd_tf_idf = TruncatedSVD(n_components=30)\n",
    "tips_LSA_pipeline = Pipeline([('tfidf', tips_vectorizer), \n",
    "                              ('svd', tips_svd_tf_idf)])\n",
    "\n",
    "# Train the LSA model\n",
    "tips_LSA_pipeline.fit(all_tip_texts)\n",
    "\n",
    "# Memory management\n",
    "del all_tip_texts\n",
    "gc.collect()\n",
    "print(f'Training Latent Semantic Analysis Pipeline on Tips: Stage Time: {time.time() - stage_time:.0f} seconds. Total Time: {time.time() - start_time:.0f} seconds.')\n",
    "print(f'tips_LSA_pipeline size: {len(tips_LSA_pipeline[\"tfidf\"].get_feature_names())} -> {tips_LSA_pipeline[\"svd\"].components_.shape[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "automatic-guess",
   "metadata": {},
   "source": [
    "#### Extract Tips Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "declared-steal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting Tips Features...\n",
      "Extracting Tips Features: Stage Time: 91 seconds. Total Time: 401 seconds.\n"
     ]
    }
   ],
   "source": [
    "print(f'Extracting Tips Features...')\n",
    "stage_time = time.time()\n",
    "VADER_sentiment_analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Extract tips Features with User/Businesses as keys\n",
    "tips_features = sc.textFile(folder_path+'tip.json') \\\n",
    "    .map(lambda row: json.loads(row.replace('\\\\n', ''))) \\\n",
    "    .filter(lambda row: row['user_id'] in distinct_user) \\\n",
    "    .filter(lambda row: row['business_id'] in distinct_bizz) \\\n",
    "    .map(lambda row: ((user_to_encoding[row['user_id']], bizz_to_encoding[row['business_id']]), (np.array(list(VADER_sentiment_analyzer.polarity_scores(row['text']).values()), dtype=np.float16), \n",
    "                                                                                                 tips_LSA_pipeline.transform([row['text']])[0].astype(np.float32),\n",
    "                                                                                                 np.array([int(row['date'][2:4])], dtype=np.int8)))) \\\n",
    "    .collectAsMap()\n",
    "\n",
    "print(f'Extracting Tips Features: Stage Time: {time.time() - stage_time:.0f} seconds. Total Time: {time.time() - start_time:.0f} seconds.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "announced-square",
   "metadata": {},
   "source": [
    "## Build Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "resistant-steps",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_augmented_features_train(row):\n",
    "\t\n",
    "\t# Hyperparameters -- Apply dropout to a certain share of features (set by DROPOUT_RATE). For all non-dropped features, apply additive/multiplicative noise of varying levels.\n",
    "\tNOISE_HIGH = 1.75 # 1.5 # 2.0\n",
    "\tNOISE_MEDIUM = 0.033 # 0.033 # 0.5 # 0.05\n",
    "\tNOISE_LOW = 0.033 # 0.1 # 0.025\n",
    "\n",
    "\t### Averages ###\n",
    "\ttry: \n",
    "\t\tuser_avg_X = user_avg_rating[row[0][0]] + np.random.normal(loc=0, scale=NOISE_HIGH)\n",
    "\t\tuser_avg_X = np.array([min(max(user_avg_X, 1.0), 5.0)], dtype=np.float32) # Prevent noisy features from going over the natural limits\n",
    "\texcept:\n",
    "\t\tuser_avg_X = np.array([np.NaN], dtype=np.float32)\n",
    "\n",
    "\ttry: \n",
    "\t\tbizz_avg_X = bizz_avg_rating[row[0][1]] + np.random.normal(loc=0, scale=NOISE_HIGH)\n",
    "\t\tbizz_avg_X = np.array([min(max(bizz_avg_X, 1.0), 5.0)], dtype=np.float32) # Prevent noisy features from going over the natural limits\n",
    "\texcept:\n",
    "\t\tbizz_avg_X = np.array([np.NaN], dtype=np.float32)\n",
    "\n",
    "\t### User & Business Complimentary Features ###\n",
    "\ttry:\n",
    "\t\tuser_X = user_features[row[0][0]]\n",
    "\t\tuser_X = np.array([min(max(user_X[0] + np.random.normal(loc=0, scale=NOISE_MEDIUM), 1.0), 5.0), user_X[1], user_X[2], user_X[3], user_X[4], user_X[5], user_X[6], user_X[7], user_X[8]], dtype=np.float32)\n",
    "\texcept:\n",
    "\t\tuser_X = np.array([np.NaN, np.NaN, np.NaN, np.NaN, np.NaN, np.NaN, np.NaN, np.NaN, np.NaN], dtype=np.float32)\n",
    "\n",
    "\ttry:\n",
    "\t\tbizz_X = bizz_features[row[0][1]][0:5]\n",
    "\t\tbizz_X = np.array([min(max(bizz_X[0] + np.random.normal(loc=0, scale=NOISE_MEDIUM), 1.0), 5.0), bizz_X[1], bizz_X[2], bizz_X[3], bizz_X[4]], dtype=np.float32)\n",
    "\texcept:\n",
    "\t\tbizz_X = np.array([np.NaN, np.NaN, np.NaN, np.NaN, np.NaN], dtype=np.float32)\n",
    "\n",
    "\ttry:\n",
    "\t\tcategories_X = np.array([cat in bizz_features[row[0][1]][5] for cat in yelp_categories], dtype=np.float32)\n",
    "\texcept:\n",
    "\t\tcategories_X = np.array([np.NaN for _ in range(len(yelp_categories))], dtype=np.float32)\n",
    "\t\n",
    "\t### Iem & User CF Features ###\n",
    "\ttry: \n",
    "\t\titem_CF_X = item_CF_feature_train[row[0][0], row[0][1]] + np.random.normal(loc=0, scale=NOISE_HIGH)\n",
    "\t\titem_CF_X = np.array([min(max(item_CF_X, 1.0), 5.0)], dtype=np.float32) # Prevent noisy features from going over the natural limits\n",
    "\texcept:\n",
    "\t\titem_CF_X = np.array([np.NaN], dtype=np.float32)\n",
    "\t\n",
    "\t### Tips Textual Features ###\n",
    "\ttry:\n",
    "\t\ttip_sentiment_X = tips_features[row[0][0], row[0][1]][0]\n",
    "\t\ttip_sentiment_X = np.array([min(max(feat + np.random.normal(loc=0, scale=NOISE_LOW), -1.0), 1.0) for feat in tip_sentiment_X.tolist()], dtype=np.float32)\n",
    "\texcept:\n",
    "\t\ttip_sentiment_X = np.array([np.NaN, np.NaN, np.NaN, np.NaN], dtype=np.float32)\n",
    "\t\n",
    "\ttry:\n",
    "\t\ttip_LSA_X = tips_features[row[0][0], row[0][1]][1]\n",
    "\texcept:\n",
    "\t\ttip_LSA_X = np.array([np.NaN for _ in range(tips_LSA_pipeline['svd'].components_.shape[0])], dtype=np.float32)\n",
    "\t\n",
    "\ttry:\n",
    "\t\ttip_date_X = tips_features[row[0][0], row[0][1]][2]\n",
    "\texcept:\n",
    "\t\ttip_date_X = np.array([np.NaN], dtype=np.float32)\n",
    "\n",
    "\t### Interaction Terms ###\n",
    "\ttry:\n",
    "\t\tinteraction_X = np.array([user_X[0]*user_X[1], user_X[0]*user_X[2], user_X[0]*user_X[7], user_X[0]*bizz_X[0], user_X[0]*bizz_X[1], \n",
    "\t\t\t                                           user_X[1]*user_X[2], user_X[1]*user_X[7], user_X[1]*bizz_X[0], user_X[1]*bizz_X[1], \n",
    "\t\t\t                                           \t\t\t\t\t\tuser_X[2]*user_X[7], user_X[2]*bizz_X[0], user_X[2]*bizz_X[1],\n",
    "\t\t\t                                           \t\t\t\t\t\t\t\t\t\t\t user_X[7]*bizz_X[0], user_X[7]*bizz_X[1],\n",
    "\t\t\t                                                                                                          bizz_X[0]*bizz_X[1]], dtype=np.float32)\n",
    "\texcept:\n",
    "\t\tinteraction_X = np.array([np.NaN, np.NaN, np.NaN, np.NaN, np.NaN, np.NaN, np.NaN, np.NaN, np.NaN, np.NaN, np.NaN, np.NaN, np.NaN, np.NaN, np.NaN], dtype=np.float32)\n",
    "\n",
    "\tX = np.concatenate([user_avg_X, bizz_avg_X, user_X, bizz_X, categories_X, item_CF_X, tip_sentiment_X, tip_LSA_X, tip_date_X, interaction_X]).astype(np.float32) \n",
    "\t'''\n",
    "\tif row[0][0]%100000 == 0:\n",
    "\t\tprint('Train sample: ', X)\n",
    "\t'''\n",
    "\treturn X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "metallic-glass",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_augmented_features_test(row):\n",
    "\n",
    "\t### Averages ###\n",
    "\ttry:\n",
    "\t\tuser_avg_X = np.array([user_avg_rating[row[0][0]]], dtype=np.float32)\n",
    "\texcept:\n",
    "\t\tuser_avg_X = np.array([np.NaN], dtype=np.float32)\n",
    "\n",
    "\ttry:\n",
    "\t\tbizz_avg_X = np.array([bizz_avg_rating[row[0][1]]], dtype=np.float32)\n",
    "\texcept:\n",
    "\t\tbizz_avg_X = np.array([np.NaN], dtype=np.float32)\n",
    "\n",
    "\t### User & Business Complimentary Features ###\n",
    "\ttry:\n",
    "\t\tuser_X = user_features[row[0][0]]\n",
    "\t\tuser_X = np.array([user_X[0], user_X[1], user_X[2], user_X[3], user_X[4], user_X[5], user_X[6], user_X[7], user_X[8]], dtype=np.float32)\n",
    "\texcept:\n",
    "\t\tuser_X = np.array([np.NaN, np.NaN, np.NaN, np.NaN, np.NaN, np.NaN, np.NaN, np.NaN, np.NaN], dtype=np.float32)\n",
    "\n",
    "\ttry:\n",
    "\t\tbizz_X = np.array(bizz_features[row[0][1]][0:5], dtype=np.float32)\n",
    "\t\tbizz_X = np.array([bizz_X[0], bizz_X[1], bizz_X[2], bizz_X[3], bizz_X[4]], dtype=np.float32)\n",
    "\texcept:\n",
    "\t\tbizz_X = np.array([np.NaN, np.NaN, np.NaN, np.NaN, np.NaN], dtype=np.float32)\n",
    "\n",
    "\ttry:\n",
    "\t\tcategories_X = np.array([cat in bizz_features[row[0][1]][5] for cat in yelp_categories], dtype=np.float32)\n",
    "\texcept:\n",
    "\t\tcategories_X = np.array([np.NaN for _ in range(len(yelp_categories))], dtype=np.float32)\n",
    "\t\n",
    "\t### Iem & User CF Features ###\n",
    "\ttry:\n",
    "\t\titem_CF_X = item_CF_feature_test[row[0][0], row[0][1]]\n",
    "\t\titem_CF_X = np.array([item_CF_X], dtype=np.float32)\n",
    "\t\t#item_CF_X = [min(max(item_CF_X, 1.0), 5.0)]\n",
    "\texcept:\n",
    "\t\titem_CF_X = np.array([np.NaN], dtype=np.float32)\n",
    "\t\n",
    "\t### Tips Textual Features ###\n",
    "\ttry:\n",
    "\t\ttip_sentiment_X = tips_features[row[0][0], row[0][1]][0]\n",
    "\texcept:\n",
    "\t\ttip_sentiment_X = np.array([np.NaN, np.NaN, np.NaN, np.NaN], dtype=np.float32)\n",
    "\t\n",
    "\ttry:\n",
    "\t\ttip_LSA_X = tips_features[row[0][0], row[0][1]][1]\n",
    "\texcept:\n",
    "\t\ttip_LSA_X = np.array([np.NaN for _ in range(tips_LSA_pipeline['svd'].components_.shape[0])], dtype=np.float32)\n",
    "\t\n",
    "\ttry:\n",
    "\t\ttip_date_X = tips_features[row[0][0], row[0][1]][2]\n",
    "\texcept:\n",
    "\t\ttip_date_X = np.array([np.NaN], dtype=np.float32)\n",
    "\t\n",
    "\t### Interaction Terms ###\n",
    "\ttry:\n",
    "\t\tinteraction_X = np.array([user_X[0]*user_X[1], user_X[0]*user_X[2], user_X[0]*user_X[7], user_X[0]*bizz_X[0], user_X[0]*bizz_X[1], \n",
    "\t\t\t                                           user_X[1]*user_X[2], user_X[1]*user_X[7], user_X[1]*bizz_X[0], user_X[1]*bizz_X[1], \n",
    "\t\t\t                                           \t\t\t\t\t\tuser_X[2]*user_X[7], user_X[2]*bizz_X[0], user_X[2]*bizz_X[1],\n",
    "\t\t\t                                           \t\t\t\t\t\t\t\t\t\t\t user_X[7]*bizz_X[0], user_X[7]*bizz_X[1],\n",
    "\t\t\t                                                                                                          bizz_X[0]*bizz_X[1]], dtype=np.float32)\n",
    "\texcept:\n",
    "\t\tinteraction_X = np.array([np.NaN, np.NaN, np.NaN, np.NaN, np.NaN, np.NaN, np.NaN, np.NaN, np.NaN, np.NaN, np.NaN, np.NaN, np.NaN, np.NaN, np.NaN], dtype=np.float32)\n",
    "\n",
    "\tX = np.concatenate([user_avg_X, bizz_avg_X, user_X, bizz_X, categories_X, item_CF_X, tip_sentiment_X, tip_LSA_X, tip_date_X, interaction_X]).astype(np.float32) \n",
    "\t'''\n",
    "\tif row[0][0]%10000 == 0:\n",
    "\t\tprint('Test sample: ', X)\n",
    "\t'''\n",
    "\treturn X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "warming-murder",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Datasets...\n",
      "Building Datasets: Stage Time: 27 seconds. Total Time: 429 seconds.\n"
     ]
    }
   ],
   "source": [
    "print(f'Building Datasets...')\n",
    "stage_time = time.time()\n",
    "\n",
    "# Feature Engineering\n",
    "#mice_imputer = IterativeImputer()\n",
    "#polynomial_features = PolynomialFeatures(degree=2)\n",
    "#svd_all_features = TruncatedSVD(n_components=25)\n",
    "#standard_scaler = StandardScaler()\n",
    "\n",
    "# Get train data features to fit the feature transformers\n",
    "#X_train = trainRDD_enc.map(lambda row: get_augmented_features_train(row)).collect()\n",
    "#X_train = np.array(X_train)\n",
    "#X_train = mice_imputer.fit_transform(X_train)\n",
    "#X_train = polynomial_features.fit_transform(X_train)\n",
    "#X_train = svd_all_features.fit_transform(X_train)\n",
    "#X_train = standard_scaler.fit_transform(X_train)\n",
    "\n",
    "# Train labels\n",
    "y_train = trainRDD_enc.map(lambda row: row[1]).collect()\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "# Get test data features\n",
    "X_test  = validRDD_enc.map(lambda row: get_augmented_features_test(row)).collect()\n",
    "X_test  = np.array(X_test)\n",
    "#X_test = mice_imputer.transform(X_test)\n",
    "#X_test = polynomial_features.transform(X_test)\n",
    "#X_test = svd_all_features.transform(X_test)\n",
    "#X_test = standard_scaler.transform(X_test)\n",
    "\n",
    "# Test labels\n",
    "y_test  = validRDD_enc.map(lambda row: row[1]).collect()\n",
    "y_test  = np.array(y_test)\n",
    "print(f'Building Datasets: Stage Time: {time.time() - stage_time:.0f} seconds. Total Time: {time.time() - start_time:.0f} seconds.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "applied-integrity",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "complex-commander",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training...\n",
      "    Epoch 1/5...\n",
      "[0]\tvalidation_0-rmse:3.44218\tvalidation_1-rmse:2.79743\n",
      "[1]\tvalidation_0-rmse:3.28469\tvalidation_1-rmse:2.77264\n",
      "[2]\tvalidation_0-rmse:3.12698\tvalidation_1-rmse:2.74939\n",
      "[3]\tvalidation_0-rmse:2.98407\tvalidation_1-rmse:2.60255\n",
      "[4]\tvalidation_0-rmse:2.83566\tvalidation_1-rmse:2.55433\n",
      "[5]\tvalidation_0-rmse:2.70181\tvalidation_1-rmse:2.52887\n",
      "[6]\tvalidation_0-rmse:2.57601\tvalidation_1-rmse:2.41720\n",
      "[7]\tvalidation_0-rmse:2.47990\tvalidation_1-rmse:2.33742\n",
      "[8]\tvalidation_0-rmse:2.36718\tvalidation_1-rmse:2.30936\n",
      "[9]\tvalidation_0-rmse:2.24984\tvalidation_1-rmse:2.23048\n",
      "    Epoch 1/5. Epoch Time: 22 seconds. Total Time: 690 seconds.\n",
      "    Epoch 2/5...\n",
      "[0]\tvalidation_0-rmse:2.18781\tvalidation_1-rmse:2.14199\n",
      "[1]\tvalidation_0-rmse:2.07776\tvalidation_1-rmse:2.11485\n",
      "[2]\tvalidation_0-rmse:1.99156\tvalidation_1-rmse:2.09409\n",
      "[3]\tvalidation_0-rmse:1.91508\tvalidation_1-rmse:2.00063\n",
      "[4]\tvalidation_0-rmse:1.84316\tvalidation_1-rmse:1.94368\n",
      "[5]\tvalidation_0-rmse:1.75813\tvalidation_1-rmse:1.92054\n",
      "[6]\tvalidation_0-rmse:1.67792\tvalidation_1-rmse:1.88253\n",
      "[7]\tvalidation_0-rmse:1.60476\tvalidation_1-rmse:1.85764\n",
      "[8]\tvalidation_0-rmse:1.54234\tvalidation_1-rmse:1.87476\n",
      "[9]\tvalidation_0-rmse:1.47035\tvalidation_1-rmse:1.84415\n",
      "    Epoch 2/5. Epoch Time: 21 seconds. Total Time: 711 seconds.\n",
      "    Epoch 3/5...\n",
      "[0]\tvalidation_0-rmse:1.51348\tvalidation_1-rmse:1.74501\n",
      "[1]\tvalidation_0-rmse:1.44240\tvalidation_1-rmse:1.68336\n",
      "[2]\tvalidation_0-rmse:1.39893\tvalidation_1-rmse:1.67411\n",
      "[3]\tvalidation_0-rmse:1.35412\tvalidation_1-rmse:1.69641\n",
      "[4]\tvalidation_0-rmse:1.30728\tvalidation_1-rmse:1.67872\n",
      "[5]\tvalidation_0-rmse:1.25581\tvalidation_1-rmse:1.68151\n",
      "[6]\tvalidation_0-rmse:1.20650\tvalidation_1-rmse:1.63677\n",
      "[7]\tvalidation_0-rmse:1.15982\tvalidation_1-rmse:1.63718\n",
      "[8]\tvalidation_0-rmse:1.12154\tvalidation_1-rmse:1.61798\n",
      "[9]\tvalidation_0-rmse:1.09052\tvalidation_1-rmse:1.59600\n",
      "    Epoch 3/5. Epoch Time: 22 seconds. Total Time: 734 seconds.\n",
      "    Epoch 4/5...\n",
      "[0]\tvalidation_0-rmse:1.11068\tvalidation_1-rmse:1.56592\n",
      "[1]\tvalidation_0-rmse:1.06668\tvalidation_1-rmse:1.52522\n",
      "[2]\tvalidation_0-rmse:1.03497\tvalidation_1-rmse:1.47445\n",
      "[3]\tvalidation_0-rmse:1.00842\tvalidation_1-rmse:1.51033\n",
      "[4]\tvalidation_0-rmse:0.97116\tvalidation_1-rmse:1.48743\n",
      "[5]\tvalidation_0-rmse:0.93958\tvalidation_1-rmse:1.49815\n",
      "[6]\tvalidation_0-rmse:0.91196\tvalidation_1-rmse:1.47964\n",
      "[7]\tvalidation_0-rmse:0.87839\tvalidation_1-rmse:1.50109\n",
      "[8]\tvalidation_0-rmse:0.84628\tvalidation_1-rmse:1.46114\n",
      "[9]\tvalidation_0-rmse:0.81741\tvalidation_1-rmse:1.44266\n",
      "    Epoch 4/5. Epoch Time: 22 seconds. Total Time: 756 seconds.\n",
      "    Epoch 5/5...\n",
      "[0]\tvalidation_0-rmse:0.92617\tvalidation_1-rmse:1.42121\n",
      "[1]\tvalidation_0-rmse:0.89768\tvalidation_1-rmse:1.41057\n",
      "[2]\tvalidation_0-rmse:0.86774\tvalidation_1-rmse:1.40754\n",
      "[3]\tvalidation_0-rmse:0.84519\tvalidation_1-rmse:1.39426\n",
      "[4]\tvalidation_0-rmse:0.82890\tvalidation_1-rmse:1.38852\n",
      "[5]\tvalidation_0-rmse:0.80279\tvalidation_1-rmse:1.35919\n",
      "[6]\tvalidation_0-rmse:0.78651\tvalidation_1-rmse:1.34387\n",
      "[7]\tvalidation_0-rmse:0.76110\tvalidation_1-rmse:1.34169\n",
      "[8]\tvalidation_0-rmse:0.73359\tvalidation_1-rmse:1.34351\n",
      "[9]\tvalidation_0-rmse:0.70773\tvalidation_1-rmse:1.32964\n",
      "    Epoch 5/5. Epoch Time: 21 seconds. Total Time: 777 seconds.\n",
      "Starting Training: Stage Time: 110 seconds. Total Time: 778 seconds.\n"
     ]
    }
   ],
   "source": [
    "print(f'Starting Training...')\n",
    "stage_time = time.time()\n",
    "# Instantiate model\n",
    "model = xgb.XGBRegressor(n_jobs = -1,\n",
    "                         n_estimators = 10, \n",
    "                         learning_rate = 0.05,\n",
    "                         num_parallel_tree = 1,\n",
    "                         booster = 'gbtree',\n",
    "                         #booster = 'dart',\n",
    "                         #rate_drop = 0.25,\n",
    "                         eval_metric = 'rmse',\n",
    "                         min_child_weight = 0, \n",
    "                         min_split_loss = 0,\n",
    "                         subsample = 0.5, \n",
    "                         colsample_bytree = 0.5,\n",
    "                         max_depth = 3,\n",
    "                         reg_lambda = 0.0, \n",
    "                         reg_alpha = 0.0) \n",
    "\n",
    "# Train using a generator for the training data, to run rounds with noisy data\n",
    "EPOCHS = 5 ##7\n",
    "first_round = True\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f'    Epoch {epoch+1}/{EPOCHS}...')\n",
    "    epoch_time = time.time()\n",
    "\n",
    "    # Get train data features\n",
    "    X_train = trainRDD_enc.map(lambda row: get_augmented_features_train(row)).collect()\n",
    "    X_train = np.array(X_train)\n",
    "    #X_train = mice_imputer.transform(X_train)\n",
    "    #X_train = polynomial_features.transform(X_train)\n",
    "    #X_train = svd_all_features.transform(X_train)\n",
    "    #X_train = standard_scaler.transform(X_train)\n",
    "\n",
    "    # Train\n",
    "    if first_round:\n",
    "        model.fit(X_train, y_train, eval_set = [(X_train, y_train), (X_test, y_test)], eval_metric = 'rmse')\n",
    "        first_round = False\n",
    "    else:\n",
    "        model.fit(X_train, y_train, xgb_model = model.get_booster(), eval_set = [(X_train, y_train), (X_test, y_test)], eval_metric = 'rmse')\n",
    "\n",
    "    print(f'    Epoch {epoch+1}/{EPOCHS}. Epoch Time: {time.time() - epoch_time:.0f} seconds. Total Time: {time.time() - start_time:.0f} seconds.')\n",
    "\n",
    "    # Memory management\n",
    "    del X_train\n",
    "    gc.collect()\n",
    "\n",
    "# Memory management\n",
    "del user_avg_rating, bizz_avg_rating, user_features, bizz_features\n",
    "gc.collect()\n",
    "print(f'Starting Training: Stage Time: {time.time() - stage_time:.0f} seconds. Total Time: {time.time() - start_time:.0f} seconds.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thirty-program",
   "metadata": {},
   "source": [
    "## Generate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aging-packing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Predictions...\n",
      "Writing Predictions: Stage Time: 0 seconds. Total Time: 778 seconds.\n"
     ]
    }
   ],
   "source": [
    "print(f'Writing Predictions...')\n",
    "stage_time = time.time()\n",
    "# Predict\n",
    "predictions = model.predict(X_test) #, ntree_limit=75*10)\n",
    "\n",
    "# Bind predictions to [1, 5]\n",
    "predictions = [min(max(pred, 1.0), 5.0) for pred in predictions]\n",
    "\n",
    "# Reverse the ID-Encoding dicts\n",
    "encoding_to_user = {v: k for k, v in user_to_encoding.items()}\n",
    "encoding_to_bizz = {v: k for k, v in bizz_to_encoding.items()}\n",
    "\n",
    "# Output predictions\n",
    "with open(output_file_name, \"w\") as fout:\n",
    "    fout.write(\"user_id, business_id, prediction\")\n",
    "    for idx, row in enumerate(validRDD_enc.collect()):\n",
    "        fout.write(\"\\n\" + f\"{encoding_to_user[row[0][0]]},{encoding_to_bizz[row[0][1]]},\"+str(predictions[idx]))\n",
    "print(f'Writing Predictions: Stage Time: {time.time() - stage_time:.0f} seconds. Total Time: {time.time() - start_time:.0f} seconds.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noticed-intent",
   "metadata": {},
   "source": [
    "## Evaluate Recommender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "suspected-vegetable",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Recommender...\n",
      "\n",
      "Error Distribution:\n",
      ">=0 and <1: 4\n",
      ">=1 and <2: 5\n",
      ">=2 and <3: 2\n",
      ">=3 and <4: 0\n",
      ">=4: 0\n",
      "\n",
      "RMSE:\n",
      "1.3296387013984001\n",
      "\n",
      "Duration: 811.9390385150909\n"
     ]
    }
   ],
   "source": [
    "print(f'Evaluating Recommender...')\n",
    "\n",
    "# Join the ground truth with predictions\n",
    "predictionsRDD = sc.textFile(output_file_name)\n",
    "predictionsHeader = predictionsRDD.first()\n",
    "predictionsRDD = predictionsRDD.filter(lambda row: row != predictionsHeader).map(lambda row: row.split(',')).map(lambda row: ((row[0],row[1]), float(row[2]))).persist()\n",
    "evaluation = validRDD.join(predictionsRDD)\n",
    "\n",
    "# Report error distribution\n",
    "delta = evaluation.map(lambda row: abs(row[1][0] - row[1][1]))\n",
    "delta_0_1 = delta.filter(lambda abs_err:      abs_err < 1).count()\n",
    "delta_1_2 = delta.filter(lambda abs_err: 1 <= abs_err < 2).count()\n",
    "delta_2_3 = delta.filter(lambda abs_err: 2 <= abs_err < 3).count()\n",
    "delta_3_4 = delta.filter(lambda abs_err: 3 <= abs_err < 4).count()\n",
    "delta_4_5 = delta.filter(lambda abs_err: 4 <= abs_err    ).count()\n",
    "print('\\n' + 'Error Distribution:')\n",
    "print(f'>=0 and <1: {delta_0_1}')\n",
    "print(f'>=1 and <2: {delta_1_2}')\n",
    "print(f'>=2 and <3: {delta_2_3}')\n",
    "print(f'>=3 and <4: {delta_3_4}')\n",
    "print(f'>=4: {delta_4_5}')\n",
    "\n",
    "# Report RMSE\n",
    "RMSE = (delta.map(lambda x: x ** 2).mean()) ** 0.5\n",
    "print('\\n' + 'RMSE:')\n",
    "print(f'{RMSE}')\n",
    "\n",
    "# Close spark context\n",
    "sc.stop()\n",
    "\n",
    "# Measure the total time taken and report it\n",
    "time_elapsed = time.time() - start_time\n",
    "print('\\n' + f'Duration: {time_elapsed}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlikely-processor",
   "metadata": {},
   "source": [
    "# The End\n",
    "Matheus Schmitz\n",
    "<br><a href=\"https://www.linkedin.com/in/matheusschmitz/\">LinkedIn</a></br>\n",
    "<br><a href=\"https://matheus-schmitz.github.io/\">Github brortfolio</a></br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
